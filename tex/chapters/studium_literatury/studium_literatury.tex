\chapter{Studium Literatury}

\section*{Streszczenie}

W studium literatury zajmę się opisaniem teorii CAP\cite{brewers-conjecture} (\emph{Consistency, Availability, Partition Tolerance}), zwanej również teorią Brewer'a od nazwiska jej autora. 
Teoria ta twierdzi, że rozproszony system nie jest w stanie zapewnić równocześnie wszystkich trzech gwarancji: konsystencji danych widzianych przez węzły systemu, odporności całości systemu na awarie poszczególnych jego węzłów oraz odporności systemu na utratę połączenia pomiędzy poszczególnymi węzłami lub ich grupami. 
Przedstawię także zasady działania Google BigTable\cite{google-bigtable} oraz Amazon Dynamo\cite{amazon-dynamo}, które bardzo istotnie wpłynęły na architekturę systemów NoSQL. 
Porównam także semantykę BASE (\emph{Basically Available, Soft-state, Eventually-consistent}) z ACID (\emph{Atomicity, Consistency, Isolation, Durability}).

\section{Teoria CAP}

Teoria CAP (\emph{Consistency, Availability, Partition Tolerance}) zwana również teorią Brewera od nazwiska jej autora została po raz pierwszy zaprezentowana podczas prezentacji profesora Uniwersytetu Berkley Eryka Brewera 19 czerwca 2000r. na konferencji \emph{ACM Symposium on the Principles of Distributed Computing}\cite{podc-keynote}. 
W około dwa lata później, w 2002 roku, teoria ta (pod nazwą \emph{Brewer's Conjecture} - Domysł Brewera) została formalnie udowodniona przez Nancy Lynch oraz Setha Gilberta z MIT\cite{brewers-conjecture}.

Teoria CAP powstała jako efekt doświadczeń Brewera w firmie Inktomi oraz jego prac badawczych nad systemami rozproszonymi na Uniwersytecie w Berkley. 
Mówi ona, że z trzech pożądanych właściwości systemu rozproszonego: Konsystencji (ang. \emph{Consistency}), Wysokiej Dostępności (ang. \emph{Availability}) oraz Odporności na Podział Sieci (ang. \emph{Partition Tolerance}) możliwe jest zapewnienie co najwyżej dwóch z nich\cite{browne-cap-theorem}.

\subsection*{Konsystencja}

Termin ,,Konsystencja'' w Teorii CAP ma nieco inne znaczenie niż w ACID, gdzie oznacza on, iż zapisywane dane nie mogą złamać pewnych określonych reguł integralności. 
W Teorii CAP \emph{Consistency} jest dużo bardziej zbliżone do \emph{Atomicity} z ACID, oznacza ono bowiem, że gdy dokonamy operacji zapisu $x=x_0$ każdy kolejny odczyt $x$, niezależnie do którego węzła byłby skierowany, zwróci wartość $x_0$.

\subsection*{Wysoka Dostępność}

Wysoka dostępność jest najbardziej pożądaną właściwością z trzech wymienionych.
Oznacza ona, jak sama nazwa wskazuje, że system powinien udostępniać swoje usługi w pełni przez cały czas, wliczając w to awarie poszczególnych węzłów, aktualizacje oprogramowania czy awarie sieci. 
Bardziej formalnie: jeżeli operacja dotrze do nie ulegającego właśnie awarii węzła, to w pewnym skończonym czasie zwróci wynik do klienta.
Warto przy tym zwrócić uwagę (za \cite{brewers-conjecture}), że dostępność zawodzi najczęściej właśnie wtedy, gdy jest najbardziej potrzebna, czyli w okresach największego obciążenia systemu.

\subsection*{Odporność na Podział Sieci}

W systemach rozproszonych, działających na tysiącach węzłów, często rozsianych w centrach danych\todo{Data Centers - jak się tłumaczy?}\ na wielu kontynentach utrata połączenia pomiędzy grupami węzłów jest oczekiwanym, codziennym problemem. 
Wyobraźmy sobie bazę danych replikowaną w systemie master-master.
Jeżeli połączenie między węzłami zostanie zerwane, modyfikacje dokonane na jednym z nich nie będą widoczne na drugim. 
Z kolei w sytuacji gdy mamy bazę danych z horyzontalnym podziałem danych (ang. \emph{sharded}), ponieważ każdy z serwerów zawiera informacje dotyczące tylko części danych i z góry wiadomo do którego z nich należy się zwrócić aby otrzymać informacje dotyczące dowolnego klucza, nawet w przypadku utraty połączenia między nimi, o ile klient ma dostęp do obu serwerów, ciągłość dostarczanych usług jest zapewniona.

\subsection*{Znaczenie Teorii}

Teoria CAP nabiera znaczenia w miarę wzrostu wielkości systemu. 
Gdy dysponujemy bazą rozsianą na kilku maszynach, narzut czasowy replikacji danych pomiędzy nimi jest akceptowalny dla większości zastosowań, nie musimy się też zbytnio martwić o podział sieci, gdyż zazwyczaj jest tak, że maszyny zlokalizowane w jednej szafie (ang. \emph{rack}) będą albo działać wszystkie, albo żadna. 
Kiedy jednak zajmujemy się jednak usługami rozproszonymi na tysiącach węzłów, nawet gdybyśmy dysponowali 10000 maszynami o niezawodności MTBF 30 lat, każdego dnia następowałaby awaria którejś z nich\cite{google-lessons}. 
W przypadku tak dużych systemów, czas jakiego wymaga replikacja danych aby doprowadzić aby każdy węzeł widział ten sam stan, czy to jak system reaguje na podziały sieci nabiera o wiele większego znaczenia.

\subsubsection*{Podział systemów}

Teoria CAP mówi, że nie możemy zapewnić równocześnie wszystkich trzech gwarancji, dlatego skalowalne systemy muszą porzucić jedną z nich. 
Ze względu na to dzielimy je na:

\begin{enumerate}
 \item \emph{CA} - (Consistent, Available) te systemy mają problemy z podziałem sieci, wymagając zazwyczaj aby operacje dotyczące poszczególnych transakcji trafiały do pojedynczej grupy węzłów, które podlegają awarii ,,atomowo'' - albo wszystkie działają, albo żaden. 
 To podejście zazwyczaj wiąże się z problemami dla skalowalności.
 \item \emph{AP} - (Available, Partition-Tolerant) te systemy zapewniają największą odporność na awarie wynikające z rozproszonego środowiska, jednocześnie jednak stawiając twórców aplikacji przed trudnym zadaniem radzenia sobie z problemami wynikającymi z niespójności danych widzianych przez klientów bazy.
 \item \emph{AP} - (Consistent, Partition-Tolerant) te systemy w wypadku podziału oczekują na przywrócenie połączenia, ograniczając w ten sposób dostępność.
\end{enumerate}

W praktyce systemy omawiane w tej pracy zazwyczaj zadowalają się częściowym zapewnieniem wszystkich wymienionych gwarancji, przy czym wysoka dostępność odgrywa główną rolę, a poświęcana jest albo odporność na podziały, albo konsystencja. 
Dlatego systemy te należą albo do grupy CA (Google BigTable, HBase, Cassandra), albo do grupy AP (Amazon SimpleDB).
\todo{Podział na CA, AP, CP - po lepszym poznaniu poszczególnych rozwiązań} 

\section{BASE}

BASE (\emph{Basically Available, Soft state, Eventual consistency}) to model konsystencji, który jest zazwyczaj przeciwstawiany modelowi ACID (\emph{Atomicity, Consistency, Isolation, Durability}). 
Jak już sama nazwa wskazuje (ang. \emph{acid} - kwas, \emph{base} - zasada), model ten znajduje się po przeciwnej stronie spektrum w stosunku do ACID.

Producenci relacyjnych baz danych od dawna już byli świadomi potrzeby partycjonowania danych na wiele węzłów.
Aby zapewnić semantykę ACID w kontekście rozproszonych transakcji stosuje się technikę 2PC (ang. \emph{2 Phase Commit}).
Protokół 2PC działa dwustopniowo:

\begin{enumerate}
 \item Najpierw koordynator transakcji żąda od wszystkich węzłów biorących udział w operacji aby wstępnie zcommitowały\todo{skomitowały?} transakcję i potwierdziły możliwość wykonania tej operacji.
 Jeżeli wszystkie węzły dokonały tego potwierdzenia, przechodzi się do drugiego kroku.
 \item W drugim kroku koordynator żąda od wszystkich zainteresowanych węzłów dokonania operacji commit.
 Jeżeli którakolwiek z baz zawetuje tą operację, wszystkie muszą wycofać transakcję.
\end{enumerate}

Problem, na jaki napotykamy w tym podejściu, to ograniczenie dostępności systemu (A w CAP).
Wystarczy aby jeden z węzłów systemu podległ awarii, aby cały system stał się niedostępny dla zapisów.
Dostępność w kontekście transakcji staje się iloczynem dostępności poszczególnych węzłów systemu.
Jeżeli mamy zatem węzły o indywidualnej dostępności 99.9\% to transakcja, która obejmuje trzy z nich będzie miała dostępność ok. 99.7\% - czyli o ok. 90 minut mniejszy \emph{uptime} w skali miesiąca.\cite{base-an-acid-alternative}

Jeżeli zatem ACID oferuje nam poziom konsystencji, który można byłoby określić mianem Strong Consistency, ale kosztem dostępności, to BASE oferuje w zamian wysoką dostępność kosztem konsystencji. 

\subsection{Eventual Consistency}

Eventual Consistency (w wolnym tłumaczeniu: konsystencja po pewnym czasie, ostatecznie) to słaba forma gwarancji konsystencji, która gwarantuje jedynie, że po pewnym, możliwym do przewidzenia czasie od momentu wykonania operacji, jej efekty będą widziane przez klientów systemu, niezależnie od tego, do którego z węzłów systemu się zwrócą z zapytaniem.
Okno czasowe między operacją a propagacją jej efektów do wszystkich zainteresowanych węzłów w systemie nazywamy oknem niespójności (ang. \emph{inconsistency window}).

Choć początkowo może się wydawać, że tego typu model sprawia duże trudności w implementacji aplikacji korzystających z baz danych, które go zapewniają, w rzeczywistości tego typu interakcje napotykamy każdego dnia.
Kiedy w systemie bankowym dokonujemy przelewu z konta na konto, pieniądze znikają z bilansu jednego z nich, ale na drugim pojawiają się z pewnym opóźnieniem.
Innym przykładem może być system DNS, gdzie zmiana jest propagowana w systemie stopniowo, często oczekując na przeterminowanie cache, ale po jakimś czasie jest zauważalna u każdego klienta.

Artykuł\cite{vogels-eventually-consistent} wprowadza ponadto kilka rodzajów Eventual Consistency:

\begin{enumerate}
 \item \emph{Causal Consistency} - Jeżeli proces A wykonał jakąś operację, a następnie zakomunikował ten fakt procesowi B, to proces B będzie widział zmienione dane w taki sam sposób jak proces A, a jego zapisy nie będą wchodzić w konflikt z tą operacją.
 Proces C, któremu ta informacja nie została przekazana, będzie podlegał normalnym regułom.
 \item \emph{Read-your-writes Consistency} - Proces dokonujący operacji w kolejnych operacjach zawsze widzi rezultaty tej operacji.
 \item \emph{Session Consistency} - To praktyczna realizacja poprzedniego rodzaju konsystencji.
 W tym przypadku proces komunikuje się z systemem w kontekście sesji, w ramach której ma zapewnioną gwarancję odczytu swoich operacji.
 W przypadku awarii i konieczności nawiązania nowej sesji, gwarancje te nie przechodzą na nowo nawiązaną sesję.
 \item \emph{Monotonic Write Consistency} - W tym przypadku system gwarantuje, że operacje zostaną wykonane w tej samej kolejności, w jakiej żądania ich wykonania zostały wysłane.
 Systemy które nie oferują tej gwarancji są bardzo trudne w użyciu.
\end{enumerate}

\subsubsection*{Konfiguracja Eventual Consistency}

Powyższy artykuł\cite{vogels-eventually-consistent} wprowadził nomenklaturę stosowaną w konfiguracji konsystencji wielu systemów NoSQL (np. Cassandra, Riak).
Konfigurację tą opisują zazwyczaj trzy liczby:

\begin{enumerate}
 \item \emph{N} - liczba węzłów systemu na które zostanie zreplikowany pojedynczy rekord.
 Liczba ta jest zazwyczaj określana jako parametr konfiguracji systemu, lub podczas wydawania polecenia utworzenia ``tabeli'' (nazwanego zbioru rekordów).
 Niektóre systemy pozwalają na zmianę tej wartości w trakcie działania systemu (np. Riak), ale większość wymaga ponownego uruchomienia aplikacji w celu zastosowania tej zmiany.
 \item \emph{R} - liczba węzłów systemu, które muszą dokonać odczytu zanim wartość zostanie zwrócona do klienta.
 Czasem wartości zwrócone przez poszczególne węzły będą różne.
 Wtedy system  odpowiada albo za rozwiązanie konfliktu, albo za przekazanie wielu wersji klientowi.
 Parametr R jest najczęściej określany z osobna dla każdego zapytania.
 \item \emph{W} - liczba węzłów systemu, które muszą potwierdzić zapis aby operacja została zakończyła się sukcesem.
 Podobnie jak R jest to parametr przekazywany dla każdego zapytania.
\end{enumerate}

Jeżeli $R+W>N$, to mamy do czynienia z silną konsystencją (ang. \emph{Strong Consistency}) - każdy odczyt zwróci ostatnią zapisaną wartość.
Jeżeli $W = 0$, to zapis jest dokonywany asynchronicznie.

\subsubsection*{Znaczenie}

Eventual Consistency opisuje problem dotykający każdej rozproszonej bazy danych, niezależnie od tego czy jest to baza relacyjna, sieciowy system plików czy baza NoSQL.
Tradycyjne podejście do konsystencji w kontekście replikacji jest ograniczające.
Systemy bazodanowe najczęściej implementują rozwiązanie typu wszystko-albo-nic: operacja zapisu musi się powieść na wszystkich węzłach albo zostać cofnięta.
Podejście takie nie tylko ogranicza dostępność systemu, powoduje ono także ograniczenie możliwości decyzyjnych autorów aplikacji korzystających z tych systemów.
Nawet jeżeli system umożliwia asynchroniczną replikację, zazwyczaj jest to bardzo prymitywny mechanizm, który nie bierze pod uwagę wersjonowania rekordów i sprowadza bazę do konfiguracji R=1, W=1.

Z drugiej strony systemy takie jak Amazon Dynamo pozwalają swoim użytkownikom na pełną dowolność w konfiguracji mechanizmów persystencji.
Nawet przy $R+W>N$ mamy możliwość sterowania zachowaniem systemu: czy zapisy powinny być szybsze kosztem odczytów, czy na odwrót, czy też może gdzieś pomiędzy.
Wiele z tych systemów zapewnia \emph{Read-your-writes Consistency} co stanowi dodatkowe ułatwienie.
Możliwość konfiguracji parametrów R, W i N stopniowo staje się standardem wśród systemów NoSQL obsługujących partycjonowanie danych.
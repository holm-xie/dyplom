\chapter{Studium Literatury}

\section*{Streszczenie}

W studium literatury zajmę się opisaniem teorii CAP\cite{brewers-conjecture} (\emph{Consistency, Availability, Partition Tolerance}), zwanej również teorią Brewer'a od nazwiska jej autora. 
Teoria ta twierdzi, że rozproszony system nie jest w stanie zapewnić równocześnie wszystkich trzech gwarancji: konsystencji danych widzianych przez węzły systemu, odporności całości systemu na awarie poszczególnych jego węzłów oraz odporności systemu na utratę połączenia pomiędzy poszczególnymi węzłami lub ich grupami. 
Przedstawię także zasady działania Google BigTable\cite{google-bigtable} oraz Amazon Dynamo\cite{amazon-dynamo}, które bardzo istotnie wpłynęły na architekturę systemów NoSQL. 
Porównam także semantykę BASE (\emph{Basically Available, Soft-state, Eventually-consistent}) z ACID (\emph{Atomicity, Consistency, Isolation, Durability}).

\section{Teoria CAP}

Teoria CAP (\emph{Consistency, Availability, Partition Tolerance}) zwana również teorią Brewera od nazwiska jej autora została po raz pierwszy zaprezentowana podczas prezentacji profesora Uniwersytetu Berkley Eryka Brewera 19 czerwca 2000r. na konferencji \emph{ACM Symposium on the Principles of Distributed Computing}\cite{podc-keynote}. 
W około dwa lata później, w 2002 roku, teoria ta (pod nazwą \emph{Brewer's Conjecture} - Domysł Brewera) została formalnie udowodniona przez Nancy Lynch oraz Setha Gilberta z MIT\cite{brewers-conjecture}.

Teoria CAP powstała jako efekt doświadczeń Brewera w firmie Inktomi oraz jego prac badawczych nad systemami rozproszonymi na Uniwersytecie w Berkley. 
Mówi ona, że z trzech pożądanych właściwości systemu rozproszonego: Konsystencji (ang. \emph{Consistency}), Wysokiej Dostępności (ang. \emph{Availability}) oraz Odporności na Podział Sieci (ang. \emph{Partition Tolerance}) możliwe jest zapewnienie co najwyżej dwóch z nich\cite{browne-cap-theorem}.

\subsection*{Konsystencja}

Termin ,,Konsystencja'' w Teorii CAP ma nieco inne znaczenie niż w ACID, gdzie oznacza on, iż zapisywane dane nie mogą złamać pewnych określonych reguł integralności. 
W Teorii CAP \emph{Consistency} jest dużo bardziej zbliżone do \emph{Atomicity} z ACID, oznacza ono bowiem, że gdy dokonamy operacji zapisu $x=x_0$ każdy kolejny odczyt $x$, niezależnie do którego węzła byłby skierowany, zwróci wartość $x_0$.

\subsection*{Wysoka Dostępność}

Wysoka dostępność jest najbardziej pożądaną właściwością z trzech wymienionych.
Oznacza ona, jak sama nazwa wskazuje, że system powinien udostępniać swoje usługi w pełni przez cały czas, wliczając w to awarie poszczególnych węzłów, aktualizacje oprogramowania czy awarie sieci. 
Bardziej formalnie: jeżeli operacja dotrze do nie ulegającego właśnie awarii węzła, to w pewnym skończonym czasie zwróci wynik do klienta.
Warto przy tym zwrócić uwagę (za \cite{brewers-conjecture}), że dostępność zawodzi najczęściej właśnie wtedy, gdy jest najbardziej potrzebna, czyli w okresach największego obciążenia systemu.

\subsection*{Odporność na Podział Sieci}

W systemach rozproszonych, działających na tysiącach węzłów, często rozsianych w centrach danych\todo{Data Centers - jak się tłumaczy?}\ na wielu kontynentach utrata połączenia pomiędzy grupami węzłów jest oczekiwanym, codziennym problemem. 
Wyobraźmy sobie bazę danych replikowaną w systemie master-master.
Jeżeli połączenie między węzłami zostanie zerwane, modyfikacje dokonane na jednym z nich nie będą widoczne na drugim. 
Z kolei w sytuacji gdy mamy bazę danych z horyzontalnym podziałem danych (ang. \emph{sharded}), ponieważ każdy z serwerów zawiera informacje dotyczące tylko części danych i z góry wiadomo do którego z nich należy się zwrócić aby otrzymać informacje dotyczące dowolnego klucza, nawet w przypadku utraty połączenia między nimi, o ile klient ma dostęp do obu serwerów, ciągłość dostarczanych usług jest zapewniona.

\subsection*{Znaczenie Teorii}

Teoria CAP nabiera znaczenia w miarę wzrostu wielkości systemu. 
Gdy dysponujemy bazą rozsianą na kilku maszynach, narzut czasowy replikacji danych pomiędzy nimi jest akceptowalny dla większości zastosowań, nie musimy się też zbytnio martwić o podział sieci, gdyż zazwyczaj jest tak, że maszyny zlokalizowane w jednej szafie (ang. \emph{rack}) będą albo działać wszystkie, albo żadna. 
Kiedy jednak zajmujemy się jednak usługami rozproszonymi na tysiącach węzłów, nawet gdybyśmy dysponowali 10000 maszynami o niezawodności MTBF 30 lat, każdego dnia następowałaby awaria którejś z nich\cite{google-lessons}. 
W przypadku tak dużych systemów, czas jakiego wymaga replikacja danych aby doprowadzić aby każdy węzeł widział ten sam stan, czy to jak system reaguje na podziały sieci nabiera o wiele większego znaczenia.

\subsubsection*{Podział systemów}

Teoria CAP mówi, że nie możemy zapewnić równocześnie wszystkich trzech gwarancji, dlatego skalowalne systemy muszą porzucić jedną z nich. 
Ze względu na to dzielimy je na:

\begin{enumerate}
 \item \emph{CA} - (Consistent, Available) te systemy mają problemy z podziałem sieci, wymagając zazwyczaj aby operacje dotyczące poszczególnych transakcji trafiały do pojedynczej grupy węzłów, które podlegają awarii ,,atomowo'' - albo wszystkie działają, albo żaden. 
 To podejście zazwyczaj wiąże się z problemami dla skalowalności.
 \item \emph{AP} - (Available, Partition-Tolerant) te systemy zapewniają największą odporność na awarie wynikające z rozproszonego środowiska, jednocześnie jednak stawiając twórców aplikacji przed trudnym zadaniem radzenia sobie z problemami wynikającymi z niespójności danych widzianych przez klientów bazy.
 \item \emph{AP} - (Consistent, Partition-Tolerant) te systemy w wypadku podziału oczekują na przywrócenie połączenia, ograniczając w ten sposób dostępność.
\end{enumerate}

W praktyce systemy omawiane w tej pracy zazwyczaj zadowalają się częściowym zapewnieniem wszystkich wymienionych gwarancji, przy czym wysoka dostępność odgrywa główną rolę, a poświęcana jest albo odporność na podziały, albo konsystencja. 
Dlatego systemy te należą albo do grupy CA (Google BigTable, HBase, Cassandra), albo do grupy AP (Amazon SimpleDB).
\todo{Podział na CA, AP, CP - po lepszym poznaniu poszczególnych rozwiązań} 

\section{BASE}

BASE (\emph{Basically Available, Soft state, Eventual consistency}) to model konsystencji, który jest zazwyczaj przeciwstawiany modelowi ACID (\emph{Atomicity, Consistency, Isolation, Durability}). 
Jak już sama nazwa wskazuje (ang. \emph{acid} - kwas, \emph{base} - zasada), model ten znajduje się po przeciwnej stronie spektrum w stosunku do ACID.

Producenci relacyjnych baz danych od dawna już byli świadomi potrzeby partycjonowania danych na wiele węzłów.
Aby zapewnić semantykę ACID w kontekście rozproszonych transakcji stosuje się technikę 2PC (ang. \emph{2 Phase Commit}).
Protokół 2PC działa dwustopniowo:

\begin{enumerate}
 \item Najpierw koordynator transakcji żąda od wszystkich węzłów biorących udział w operacji aby wstępnie zcommitowały\todo{skomitowały?} transakcję i potwierdziły możliwość wykonania tej operacji.
 Jeżeli wszystkie węzły dokonały tego potwierdzenia, przechodzi się do drugiego kroku.
 \item W drugim kroku koordynator żąda od wszystkich zainteresowanych węzłów dokonania operacji commit.
 Jeżeli którakolwiek z baz zawetuje tą operację, wszystkie muszą wycofać transakcję.
\end{enumerate}

Problem, na jaki napotykamy w tym podejściu, to ograniczenie dostępności systemu (A w CAP).
Wystarczy aby jeden z węzłów systemu podległ awarii, aby cały system stał się niedostępny dla zapisów.
Dostępność w kontekście transakcji staje się iloczynem dostępności poszczególnych węzłów systemu.
Jeżeli mamy zatem węzły o indywidualnej dostępności 99.9\% to transakcja, która obejmuje trzy z nich będzie miała dostępność ok. 99.7\% - czyli o ok. 90 minut mniejszy \emph{uptime} w skali miesiąca.\cite{base-an-acid-alternative}

Jeżeli zatem ACID oferuje nam poziom konsystencji, który można byłoby określić mianem Strong Consistency, ale kosztem dostępności, to BASE oferuje w zamian wysoką dostępność kosztem konsystencji. 

\subsection{Eventual Consistency}

Eventual Consistency (w wolnym tłumaczeniu: konsystencja po pewnym czasie, ostatecznie) to słaba forma gwarancji konsystencji, która gwarantuje jedynie, że po pewnym, możliwym do przewidzenia czasie od momentu wykonania operacji, jej efekty będą widziane przez klientów systemu, niezależnie od tego, do którego z węzłów systemu się zwrócą z zapytaniem.
Okno czasowe między operacją a propagacją jej efektów do wszystkich zainteresowanych węzłów w systemie nazywamy oknem niespójności (ang. \emph{inconsistency window}).

Choć początkowo może się wydawać, że tego typu model sprawia duże trudności w implementacji aplikacji korzystających z baz danych, które go zapewniają, w rzeczywistości tego typu interakcje napotykamy każdego dnia.
Kiedy w systemie bankowym dokonujemy przelewu z konta na konto, pieniądze znikają z bilansu jednego z nich, ale na drugim pojawiają się z pewnym opóźnieniem.
Innym przykładem może być system DNS, gdzie zmiana jest propagowana w systemie stopniowo, często oczekując na przeterminowanie cache, ale po jakimś czasie jest zauważalna u każdego klienta.

Artykuł\cite{vogels-eventually-consistent} wprowadza ponadto kilka rodzajów Eventual Consistency:

\begin{enumerate}
 \item \emph{Causal Consistency} - Jeżeli proces A wykonał jakąś operację, a następnie zakomunikował ten fakt procesowi B, to proces B będzie widział zmienione dane w taki sam sposób jak proces A, a jego zapisy nie będą wchodzić w konflikt z tą operacją.
 Proces C, któremu ta informacja nie została przekazana, będzie podlegał normalnym regułom.
 \item \emph{Read-your-writes Consistency} - Proces dokonujący operacji w kolejnych operacjach zawsze widzi rezultaty tej operacji.
 \item \emph{Session Consistency} - To praktyczna realizacja poprzedniego rodzaju konsystencji.
 W tym przypadku proces komunikuje się z systemem w kontekście sesji, w ramach której ma zapewnioną gwarancję odczytu swoich operacji.
 W przypadku awarii i konieczności nawiązania nowej sesji, gwarancje te nie przechodzą na nowo nawiązaną sesję.
 \item \emph{Monotonic Write Consistency} - W tym przypadku system gwarantuje, że operacje zostaną wykonane w tej samej kolejności, w jakiej żądania ich wykonania zostały wysłane.
 Systemy które nie oferują tej gwarancji są bardzo trudne w użyciu.
\end{enumerate}

\subsubsection*{Konfiguracja Eventual Consistency}

Powyższy artykuł\cite{vogels-eventually-consistent} wprowadził nomenklaturę stosowaną w konfiguracji konsystencji wielu systemów NoSQL (np. Cassandra, Riak).
Konfigurację tą opisują zazwyczaj trzy liczby:

\begin{enumerate}
 \item \emph{N} - liczba węzłów systemu na które zostanie zreplikowany pojedynczy rekord.
 Liczba ta jest zazwyczaj określana jako parametr konfiguracji systemu, lub podczas wydawania polecenia utworzenia ``tabeli'' (nazwanego zbioru rekordów).
 Niektóre systemy pozwalają na zmianę tej wartości w trakcie działania systemu (np. Riak), ale większość wymaga ponownego uruchomienia aplikacji w celu zastosowania tej zmiany.
 \item \emph{R} - liczba węzłów systemu, które muszą dokonać odczytu zanim wartość zostanie zwrócona do klienta.
 Czasem wartości zwrócone przez poszczególne węzły będą różne.
 Wtedy system  odpowiada albo za rozwiązanie konfliktu, albo za przekazanie wielu wersji klientowi.
 Parametr R jest najczęściej określany z osobna dla każdego zapytania.
 \item \emph{W} - liczba węzłów systemu, które muszą potwierdzić zapis aby operacja została zakończyła się sukcesem.
 Podobnie jak R jest to parametr przekazywany dla każdego zapytania.
\end{enumerate}

Jeżeli $R+W>N$, to mamy do czynienia z silną konsystencją (ang. \emph{Strong Consistency}) - każdy odczyt zwróci ostatnią zapisaną wartość.
Jeżeli $W = 0$, to zapis jest dokonywany asynchronicznie.

\subsubsection*{Znaczenie}

Eventual Consistency opisuje problem dotykający każdej rozproszonej bazy danych, niezależnie od tego czy jest to baza relacyjna, sieciowy system plików czy baza NoSQL.
Tradycyjne podejście do konsystencji w kontekście replikacji jest ograniczające.
Systemy bazodanowe najczęściej implementują rozwiązanie typu wszystko-albo-nic: operacja zapisu musi się powieść na wszystkich węzłach albo zostać cofnięta.
Podejście takie nie tylko ogranicza dostępność systemu, powoduje ono także ograniczenie możliwości decyzyjnych autorów aplikacji korzystających z tych systemów.
Nawet jeżeli system umożliwia asynchroniczną replikację, zazwyczaj jest to bardzo prymitywny mechanizm, który nie bierze pod uwagę wersjonowania rekordów i sprowadza bazę do konfiguracji R=1, W=1.

Z drugiej strony systemy takie jak Amazon Dynamo pozwalają swoim użytkownikom na pełną dowolność w konfiguracji mechanizmów persystencji.
Nawet przy $R+W>N$ mamy możliwość sterowania zachowaniem systemu: czy zapisy powinny być szybsze kosztem odczytów, czy na odwrót, czy też może gdzieś pomiędzy.
Wiele z tych systemów zapewnia \emph{Read-your-writes Consistency} co stanowi dodatkowe ułatwienie.
Możliwość konfiguracji parametrów R, W i N stopniowo staje się standardem wśród systemów NoSQL obsługujących partycjonowanie danych.

\section{Google MapReduce}

Google MapReduce\cite{google-mapreduce} jest biblioteką wykorzystywaną do przetwarzania dużych zbiorów danych w środowisku rozproszonym.
Użytkownik biblioteki specyfikuje dwie funkcje nazywane \emph{map} i \emph{reduce} oraz kilka innych parametrów konfiguracyjnych.
Następnie biblioteka dba o to aby dane wejściowe zostały podzielone, na poszczególnych rekordach została wykonana funkcja \emph{map}, a jej wyniki zostały zagregowane przy pomocy funkcji \emph{reduce}.

Operacje \emph{map} i \emph{reduce} są powszechnie spotykane w językach funkcyjnych, takich jak na przykład LISP.
\emph{Map} na wejściu otrzymuje parę $(k, v): k \in K_1, v \in V_1$ a na wyjściu emituje listę par $(k, v): k \in K_2, v \in V_2$.
\emph{Reduce} na wejściu otrzymuje parę $(k, (v_1, ..., v_n)): k \in K_2, v_1...v_n \in V_2$ i na wyjściu emituje $v: v \in V_2$.

\subsection*{Przykład}

Poniżej przedstawiam przykładowy kod funkcji \emph{map} i \emph{reduce} w języku Python dla problemu zliczania wystąpień słów w zbiorze tekstów. 
Jak widzimy funkcja \emph{map} przyjmuje pary (klucz, wartość) z przestrzeni (Nazwy Dokumentów, Treść Dokumentów), zwracając pary z innej przestrzeni (Słowa, Liczby Naturalne).
Funkcja \emph{reduce} w przykładzie przyjmuje pary gdzie kluczem jest słowo, natomiast wartością jest lista liczb naturalnych określająca liczby wystąpień tego słowa w różnych dokumentach (albo wielokrotnie w tym samym dokumencie).

\begin{verbatim}
def map(document_name, document_value):
  """ funkcja mapujaca dokumenty na pary (slowo, 1)  """
  for word in words(document_value):
    yield (word, 1) # emit word

def reduce(word, counts):
  """ 
  funkcja redukujaca, przyjmuje slowo 
  i liste (iterator) po liczbie jego wystapien
  zwrace sumaryczna liczbe wystapien danego slowa
  """
  sum = 0
  for value in counts:
    sum += value
    # sum += 1
    # tak tez mozna byloby zapisac, 
    # ale wtedy funkcja nie bylaby laczna     
  return sum 
\end{verbatim}


\subsection*{Opis działania}

Użytkownik tworzy aplikację, w której specyfikuje dwie funkcje \emph{map} i \emph{reduce}, oraz dwa parametry konfiguracyjne: \emph{M} i \emph{R}:

\begin{itemize}
 \item \emph{M} - określa na ile części ma zostać podzielony plik wejściowy.
 Zazwyczaj wybiera się taką liczbę aby wielkość plików wejściowych zawierała się między 16MB a 64MB.
 Ponieważ GFS dzieli pliki na kawałki (ang. \emph{chunks}) wielkości 64MB, jest dość istotne aby pliki wejściowe nie przekraczały tej wielkości, gdyż w przeciwnym przypadku mogłaby występować konieczność komunikacji sieciowej w celu odczytania danych z pliku wejściowego.
 \emph{M} określa ponadto liczbę zadań \emph{map}.
 \item \emph{R} - określa liczbę zadań \emph{reduce}.
\end{itemize}

\missingfigure{schemat działania map reduce}

\begin{enumerate}
 \item Biblioteka MapReduce dzieli plik wejściowy na \emph{M} części.
 \item Program użytkownika zostaje wysłany i uruchomiony na maszynach klastra.
 Jedna z tych maszyn przyjmuje specjalną rolę \emph{master}, pozostałe zaś mają rolę \emph{worker}\todo{może powinienem zamienić master na zarządca/nadzorca a worker na pracownik?}.
 \item \emph{master} przypisuje poszczególnym \emph{workerom} po jednym zadaniu do wykonania.
 Kolejne są przydzielane w miarę jak węzły kończą przydzieloną im pracę.
 Zadanie \emph{map} zostanie przydzielone w pierwszej kolejności maszynie która jest równocześnie \emph{chunkserverem} przechowującym odpowiedni plik wejściowy.
 Pozwala to uniknąć komunikacji sieciowej w celu odczytania pliku.
 \item \emph{Worker} przetwarza plik wejściowy rekord po rekordzie wywołując funkcję \emph{map} i zapisując jej wynik w pamięci.
 \item Co pewien czas dane zapisane w pamięci są zrzucane na dysk do plików lokalnych.
 W tym procesie dane są rozdzielane do \emph{R} plików poprzez funkcję partycjonującą, domyślnie $hash(key) mod R$.
 Lokacje tych plików są przekazywane do węzła \emph{master}, który z kolei jest odpowiedzialny za przekazanie ich do węzła wykonującego operację \emph{reduce} na odpowiednim fragmencie danych.
 \item Węzeł wykonujący operację \emph{reduce} po otrzymaniu takiego powiadomienia pobiera odpowiednie pliki bezpośrednio od węzła, który je przechowuje.
 Po otrzymaniu wszystkich potrzebnych plików, \emph{worker} sortuje otrzymane dane po kluczu, tak aby wartości dla danego klucza sąsiadowały ze sobą w pliku.
 \item Po posortowaniu plików wejściowych węzeł iteruje po kluczach i dla każdego z nich przekazuje klucz oraz listę wszystkich przypisanych mu wartości do funkcji \emph{reduce}, zapisując następnie jej wynik w pliku wyjściowym.
 \item Kiedy wszystkie operacje \emph{reduce} zakończą się, \emph{master} budzi program użytkownika i wywołanie funkcji \emph{MapReduce} kończy się.
\end{enumerate}

Wynikiem operacji jest \emph{R} plików wynikowych.
W większości przypadków konsumentem tych danych są inne operacje MapReduce, bądź aplikacje rozproszone, więc nie ma potrzeby łączenia tych plików w jedną całość.

\subsection*{Optymalizacje}

W artykule\cite{google-mapreduce} zostało opisanych kilka istotnych optymalizacji i ulepszeń:

\begin{enumerate}
 \item Operacja \emph{map} na pliku wejściowym jest wykonywana na tym samym serwerze, który przechowuje ten plik.
 \item Operacja \emph{map} może zwrócić bardzo wiele wartości dla danego klucza pośredniego.
 Z tego względu biblioteka wprowadza pojęcie funkcji łączącej (ang. \emph{combiner}).
 Funkcja ta dokonuje wstępnej redukcji przed wysłaniem wartości przez sieć do węzła wykonującego operację \emph{reduce}.
 Najczęściej stosuje się w tym miejscu tą samą funkcję co w operacji \emph{reduce}, ale aby to było możliwe, funkcja ta musi być łączna i przemienna, co czasem wymaga wprowadzenia pewnych zmian.
 Dla przykładu: przedstawiona wcześniej funkcja map emitująca wszystkie słowa w danym dokumencie emituje powtarzające się słowa wielokrotnie.
 Poprzez zsumowanie wystąpień przed przesłaniem znacznie zmniejszamy ilość danych do wysłania.
 \item Funkcja partycjonująca może być wyspecyfikowana przez użytkownika, dzięki czemu potencjalnie możliwe jest takie jej określenie, aby dane powiązane ze sobą znalazły się w jednym pliku wynikowym (aczkolwiek kosztem ryzyka nierównomiernego podziału kluczy między partycje).
 \item Dzięki sortowaniu kluczy pośrednich, pliki wynikowe są łatwiejsze do przetwarzania, umożliwiając na przykład wyszukiwanie binarne.
 \item Kiedy zbliża się koniec operacji, zadania \emph{reduce} są zlecane do wykonania przez dodatkowe węzły.
 Dzięki temu uszkodzone, nadmiernie obciążone przez inne procesy albo wadliwie skonfigurowane maszyny nie powodują nadmiernego wydłużenia całości operacji MapReduce.
\end{enumerate}

\subsection*{Odporność na awarie}

Ponieważ biblioteka MapReduce służy do wykonywania operacji w rozproszonym środowisku, często nawet na tysiącach węzłów, konieczne jest aby była ona odporna na awarie części z węzłów.
Rozróżniamy dwa typy awarii: awaria węzła \emph{master} i awarie węzłów typu \emph{worker}.

\subsubsection*{Awaria węzła master}

Ponieważ \emph{master} przechowuje między innymi informacje o zrealizowanych zadaniach i lokalizacji plików wynikowych.
\emph{Master} musi działać aby operacja MapReduce się zakończyła powodzeniem, ale możliwe jest aby jego struktury danych były zapisywane w GFS, albo replikowane do zapasowego węzła.
W opisanym systemie awaria węzła master zawsze kończy się niepowodzeniem całości operacji MapReduce i koniecznością jej ponownego uruchomienia.
Jest to dopuszczalne ponieważ czas trwania operacji jest liczony w minutach, więc awaria tego konkretnego węzła jest mało prawdopodobna (w odróżnieniu od np. BigTable, który jest systemem, który działa bez przerwy).

\subsubsection*{Awaria węzła slave}

\emph{Master} regularnie komunikuje się z węzłami \emph{slave} w celu ustalenia ich stanu.
W przypadku gdy węzeł przestaje odpowiadać jest uznawany za ,,martwy'', w związku z czym \emph{master} oznacza zadanie aktualnie przez niego wykonywane jako przeznaczone to przydziału, tak samo wszystkie wykonane przez niego zadania \emph{map}.
Zadania \emph{map} muszą być wykonane ponownie, ponieważ ich wyniki zostały zapisane w lokalnych plikach i przez to są niedostępne do odczytu.
Zadania \emph{reduce} zapisują swoje wyniki w GFS, w związku z tym nie muszą być powtarzane. 

\subsection*{Znaczenie}

Większość systemów opisanych w niniejszej pracy nie dysponuje zaawansowanymi metodami wykonywania zapytań, a w szczególności zapytań agregujących czy zliczających takich jak funkcje COUNT, SUM, AVG i operator GROUP BY w relacyjnych bazach danych.
Operacje tego typu są szczególnie trudne w kontekście systemów rozproszonych, gdzie wykonanie zapytań tego typu w trybie on-line przy zachowaniu odpowiedniego czasu odpowiedzi jest często wręcz niemożliwe.
Dlatego najczęściej wykonywanie tego typu operacji jest dokonywane co pewien czas, lub zlecane przy zapisie danych, a jego wyniki są przechowywane w systemie.

W przeważającej większości obliczenia te są wykonywane przy zastosowaniu algorytmu MapReduce.
Wiele systemów, tak jak MongoDB czy Riak, zapewnia taką funkcjonalność, inne wymagają zastosowania zewnętrznych narzędzi.
Wierną implementacją Google MapReduce jest Hadoop, który opiszemy nieco bliżej przy okazji Hbase.
Bardzo ciekawe podejście do MapReduce prezentuje CouchDB, która przechowuje wyniki funkcji map, dzięki czemu umożliwia dokonywanie zapytań opartych o MapReduce w trybie on-line.
CouchDB zostanie opisane bliżej w kolejnym rozdziale.

Algorytm MapReduce ma duże znaczenie w systemach NoSQL, gdyż stanowi surogat dla wykonywania skomplikowanych zapytań.
Stanowi on jedno z podstawowych narzędzi dla praktyki prostych odczytów, ale złożonych zapisów.

\section{Amazon Dynamo}

W tym rozdziale opiszę Amazon Dynamo\cite{amazon-dynamo} - bazę typu klucz-wartość zaprojektowaną z myślą o skalowalności i wysokiej dostępności.
Amazon Dynamo jest dobrym przykładem systemów AP (\emph{Available, Partition Tolerant}).
W oparciu o techniki opisane w tym artykule powstał darmowy system open-source o nazwie Riak.

\section{Google BigTable}

W kolejnym rozdziale opiszę Google BigTable\cite{google-bigtable} - bazę stosowaną w Google, przewyższającą Amazon Dynamo o rząd wielkości pod względem możliwej liczby węzłów, opartą na rozproszonym systemie plików GFS.
Google BigTable jest przykładem bazy CA (\emph{Consistent, Avaliable}).
W rozdziale opisującym dostępne systemy opiszę Hbase - bazę silnie wzorowaną na BigTable, ale opartą na Hadoop Core zamiast GFS, oraz Apache Cassandra, która łączy w sobie cechy Amazon Dynamo i Google BigTable.

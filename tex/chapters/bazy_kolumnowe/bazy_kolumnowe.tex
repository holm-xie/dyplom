\chapter{Bazy Kolumnowe}

\section*{Streszczenie}
W tym rozdziale opisane zostaną dwie bazy kolumnowe: HBase i Apache Cassandra.
Mimo podobnego modelu danych, bazy te znacznie różnią się pod wieloma względami.
HBase powstało jako implementacja open source systemu opisanego w artykule o Google BigTable i mimo ciągłego rozwoju i dodawania coraz nowszych funkcjonalności te dwa systemy są do siebie wciąż bardzo zbliżone.
Z drugiej strony Apache Cassandra, który to system został zaprojektowany między innymi przez jednego z autorów Amazon Dynamo, mimo że czerpie swój model danych z Google BigTable, na poziomie architektury znacznie bardziej przypomina system firmy Amazon niż Google, a co za tym idzie podobnie jak Riak należy do grupy systemów oferujących bardzo duże gwarancje dostępności.

\section{Apache Cassandra}
\label{sec:cassandra}

\subsection*{Wstęp}

Apache Cassandra powstała na potrzeby jednej z największych stron internetowych na świecie: sieci społecznościowej Facebook. 
Zadaniem tego systemu było zastąpienie wcześniejszej architektury opartej o MySQL do wyszukiwania w skrzynkach wiadomości użytkowników.
Jednym z architektów nowego systemu był Avinash Lakshman, który wcześniej był współautorem Amazon Dynamo.
W 2008 roku Facebook zdecydował się na udostępnienie swojego produktu na licencji open source.
Przez pierwszy rok jedynie programiści firmy Facebook mieli możliwość zmiany kodu aplikacji, co w połączeniu z tym, że odmawiali oni przyjmowania zmian autorstwa innych programistów, powodowało duże napięcia w społeczności powiązanej z tym projektem i podniesienie się licznych głosów nawołujących do podziału projektu (ang. \emph{fork}).
W marcu 2009 Facebook przekazał prawa do Cassandra fundacji Apache, która okazała się dużo bardziej sprawna w zarządzaniu tym projektem\cite{evans-cassandra}.
W tym samym roku ukazała się także publikacja\footnote{Czytając ten artykuł warto jednak pamiętać, że odnosi się on do systemu w postaci w jakiej został on stworzony na potrzeby firmy Facebook, a nie do systemu, który ostatecznie został upubliczniony i nosi teraz nazwę Apache Cassandra. Główną różnicą między nimi jest to, że integracja z systemem Zookeeper nie została upubliczniona i nie jest częścią Apache Cassandra, a co za tym idzie różnią się one tym, że dostępna publicznie wersja systemu nie ma wyróżnionych węzłów i nie zależy od innych systemów.} omawiająca architekturę projektu autorstwa jego twórców \cite{cassandra-paper}.

Apache Cassandra stanowi bardzo ciekawe połączenie modelu danych zaczerpniętego z Google BigTable z modelem replikacji i partycjonowania (a co za tym idzie konsystencji) zaczerpniętym z Amazon Dynamo.
Podobnie jak opisany wcześniej Riak, Cassandra nie posiada wyróżnionych węzłów, ale w odróżnieniu od tej bazy typu klucz-wartość posiada ona bardziej zaawansowany model danych.

\subsection*{Protokół komunikacji}

Apache Cassandra posiada API wykorzystujące technologię RPC Apache Thrift\footnote{Apache Thrift został stworzony na potrzeby firmy Facebook. Jest on podobny do technologii CORBA.} do komunikacji, dzięki czemu oferuje wsparcie dla 12 języków programowania.
Dla najpopularniejszych języków programowania takich jak Java, Python czy Ruby istnieją także biblioteki wyższego poziomu, często wzorowane na bibliotekach ORM.

Cassandra pozwala na rozszerzanie podstawowego zestawu funkcjonalności systemu (na przykład zestawu komparatorów używanego do sortowania) poprzez implementację własnych klas w języku Java (w tym języku została napisana ta baza).
Dzięki temu, że istnieje wiele języków programowania, które są kompilowane do kodu pośredniego maszyny wirtualnej Java (JVM), rozszerzenia te można pisać także w wielu innych językach.

\subsection*{Replikacja}

Apache Cassandra czerpie swój model replikacji z Amazon Dynamo.
Podobnie jak w przypadku tamtej bazy użytkownik specyfikuje parametry R, W i N, określające odpowiednio ile węzłów musi odpowiedzieć na zapytanie aby odczyt się powiódł, ile musi potwierdzić zapis, oraz na ile węzłów jest replikowany dany klucz.

Bardzo ciekawą właściwością tej bazy jest to, że strategia wyboru węzłów, na które zostanie replikowany dany przedział kluczy może być konfigurowana.
Poza domyślną strategią, która podobnie jak w Amazon Dynamo replikuje klucze na N-1 kolejnych węzłów, dostępne są także dwie dodatkowe strategie: ,,Rack Aware Strategy'' oraz ,,Data Center Shard Strategy''.
Pierwsza z nich sprawia, że dla każdego przedziału drugi węzeł na liście preferencyjnej\footnote{patrz strona \pageref{sec:dynamo-replikacja}.} będzie węzłem z innego centrum obliczeniowego, a kolejne będą z innej szafy (ang. \emph{rack}) niż koordynator.
Druga z dodatkowych strategii pozwala na zdefiniowanie jak repliki mają zostać podzielone między centrami obliczeniowymi w przypadku gdy jest ich więcej niż dwa.
Dla przykładu w przypadku gdy dysponujemy trzema centrami obliczeniowymi, a każdy klucz jest replikowany na sześć węzłów, możemy zdefiniować że trzy z nich będą węzłami z pierwszego centrum, dwa z drugiego i jeden z trzeciego.

Podobnie jak Riak, Cassandra pozwala na specyfikowanie parametrów R i W dla zapytań nie tylko jako liczby, ale także jako wartości symboliczne, np. $ALL$ (wszystkie), czy $QUORUM$ (większość).
Dodatkowo dostępna jest wartość $DCQUORUM$, która działa w połączeniu ze strategiami opisanymi powyżej.
Pozwala ona na wyspecyfikowanie, że jedynie węzły z lokalnego centrum obliczeniowego mają być brane pod uwagę przy określaniu liczby węzłów potrzebnej do osiągnięcia kworum, dzięki czemu można zapewnić konsystencję na poziomie centrum danych oraz uniknąć oczekiwania na odpowiedź odległych węzłów.

\subsection*{Partycjonowanie}

Cassandra wykorzystuje algorytm \emph{Consistent Hashing}\footnote{patrz strona \pageref{sec:dynamo-consistent-hashing}.} w wersji podstawowej.
Każdemu węzłowi jest przyporządkowany jeden przedział kluczy, których jest koordynatorem, a ponadto każdy klucz jest replikowany na N-1 kolejnych węzłów.

Algorytm podziału kluczy na przedziały jest konfigurowalny.
Domyślnym (i polecanym) algorytmem jest podział ,,losowy'', czyli tak samo jak w Amazon Dynamo na podstawie funkcji mieszającej MD5, dzięki czemu klucze powinny być rozłożone równomiernie pomiędzy węzłami.
Pozostałe dwa dostępne algorytmy nie używają funkcji mieszającej, tylko porównują wartość klucza bezpośrednio z wartościami granicznymi przypisanymi poszczególnym węzłom, dzięki czemu węzły o kolejnych kluczach trafią do tego samego węzła (podobnie jak to ma miejsce w Google BigTable).
Umożliwia to wyszukiwanie zakresów kluczy na podstawie ich wartości.
Niestety to podejście może powodować bardzo nierównomierny rozkład obciążenia w klastrze, a co za tym idzie doprowadzić do niestabilnego działania systemu.
Na szczęście wykorzystując dodatkową rodzinę kolumn (patrz niżej) można własnoręcznie stworzyć indeks pozwalający na dokonywanie tego typu zapytań bez narażania się na ryzyka związanie ze zmianą algorytmu partycjonowania.

\subsection*{Persystencja}

Cassandra, podobnie jak Google BigTable\footnote{patrz strona \pageref{sec:bigtable-architektura-serwera-tabletow}.} zapisuje dane najpierw do logu transakcji, który potem może służyć do ewentualnego odtworzenia stanu bazy w razie awarii, a następnie do struktury danych w pamięci (\emph{memtable}).
Z pamięci co pewien czas dane są zrzucane na dysk tworząc pliki SSTable (Cassandra wykorzystuje tą samą nomenklaturę co Google BigTable).
Ponieważ odczyty często muszą przeglądnąć więcej niż jeden plik SSTable aby znaleźć najnowszą wersję rekordu, konieczne jest łączenie tych plików co pewien czas w większe.

Użytkownik bazy ma do dyspozycji dwie opcje konfiguracji wpływające na trwałość danych.
Możliwe jest albo wykonywanie operacji \verb+fsync()+ przed potwierdzeniem każdej operacji zapisu, albo co pewien określony czas.
W pierwszej z tych konfiguracji zaleca się aby dziennik transakcji znajdował się na innym dysku niż reszta plików, aby uniknąć opóźnienia spowodowanego przesunięciem głowicy przy każdym zapisie.

\subsection*{Wersjonowanie}

Cassandra, w odróżnieniu od Amazon Dynamo i Riak nie dysponuje mechanizmem zegarów wektorowych.
W Cassandrze każda wartość (a właściwie para klucz-nazwa kolumny) ma przypisaną 64-bitową liczbę całkowitą będącą znacznikiem czasowym ostatniej modyfikacji.
W przypadku gdy system wykryje istnienie więcej niż jednej wartości dla danej kolumny, automatycznie jest wybierana najnowsza wartość.

Brak wsparcia dla mechanizmu zegarów wektorowych wydaje się być problematyczny w systemie, który podobnie jak Amazon Dynamo czy Riak należy do rodziny systemów AP w rozumieniu teorii CAP.
Warto jednak zwrócić uwagę, że Dynamo i Riak są systemami typu klucz-wartość i tam wersjonowane są całe rekordy, a zatem jeżeli rekord zostanie zmodyfikowany przez dwóch klientów równocześnie, to nawet jeżeli ci klienci zmodyfikowali całkiem różne ,,pola''\footnote{W systemach typu klucz wartość najczęściej przypisujemy kluczowi wartość będącą zserializowanym obiektem, np. w formacie JSON. Przez pole rekordu rozumiemy tu właśnie pole tego zserializowanego obiektu.} rekordu, konflikt musi zostać rozwiązany przez aplikację.
W przypadku Cassandry każde takie ,,pole'' ma swój znacznik wersji, a co za tym idzie wiele przypadków konfliktów napotykanych w systemach typu klucz-wartość nie ma miejsca w bazie kolumnowej.
W większości pozostałych przypadków można uniknąć powstawania konfliktów odpowiednio modyfikując strukturę danych.

\subsection*{Wyszukiwanie}

\subsubsection*{Model Danych}

Model danych w Apache Cassandra bardzo przypomina ten, który został wcześniej opisany przy okazji Google BigTable\footnote{patrz strona \pageref{google-bigtable-model-danych}.}, ale z pewnymi istotnymi różnicami.
Niestety jedną z tych różnic jest wykorzystanie tej samej nomenklatury, ale w różnych znaczeniach, co może być mylące dla osób które znają model danych bazy Google, dlatego poniżej przedstawiona zostanie lista terminów dotyczących Apache Cassandra wraz z odniesieniami do odpowiadających im terminów dotyczących BigTable.
Bardzo dobre wprowadzenie do modelu danych Apache Cassandra można znaleźć w artykule na blogu Arina Sarkissiana \cite{arin-wtf-is-a-supercolumn}.

\begin{description}
 \item[Keyspace] - dosłownie ,,przestrzeń kluczy''.
 Pozwala na grupowanie rodzin kolumn tworząc pewien rodzaj ,,przestrzeni nazw'' i pozwalając na ustawienie różnych zmiennych konfiguracyjnych.
 Pod wieloma względami przypomina pojęcie bazy, znane z relacyjnych systemów zarządzania bazami danych, takich jak MySQL.
 Nie posiada odpowiednika w BigTable.

 \item[Column Family] - rodzina kolumn.
 Jest to zbiór rzędów, z których każdy posiada klucz, który go identyfikuje oraz dowolną liczbę kolumn.
 Nie istnieje żaden schemat, który wymuszałby aby rzędy tej samej rodziny kolumn miały tą samą strukturę - każdy może mieć zupełnie inny zestaw kolumn.
 Co więcej, kolumny należące do danej rodziny mogą być wyszukiwane po nazwie, stronicowane a ponadto są posortowane przy użyciu komparatora wybranego przez użytkownika.
 Te właściwości sprawiają, że bardzo często dane są przechowywane w nazwach kolumn, a wartości są ignorowane.
 W BigTable odpowiednikiem rodziny kolumn jest tabela.

 \item[Column] - kolumna.
 Jest to trójka (nazwa, wartość, czas ostatniej modyfikacji).
 Nazwa i wartość kolumny nie może przekroczyć rozmiary dwóch gigabajtów.
 W odróżnieniu od Google BigTable (gdzie jest przechowywanych wiele wersji wartości), Cassandra przechowuje tylko najnowszą wersję, a znacznik czasowy jest używany tylko przy rozstrzyganiu konfliktów.

 \item[Super Column] - super kolumna.
 Jest to struktura pozwalająca na grupowanie wielu kolumn.
 W odróżnieniu od rodziny kolumn, super kolumny nie posiadają kluczy.
 Odpowiednikiem super kolumn w BigTable są rodziny kolumn.

 \item[Super Column Family] - rodzina super kolumn.
 Zwykła rodzina kolumn nie pozwala na to aby jej rzędy składały się z kolumn i super kolumn, dopuszczalne są tylko kolumny.
 Rodzina super kolumn z kolei pozwala jedynie aby jej rzędy składały się z super kolumn.
 Stanowi to różnicę w stosunku do BigTable, gdzie rząd tabeli mógł składać się z kolumn zarówno pogrupowanych w rodziny jak i nie.
\end{description}

Do wersji 0.7 Apache Cassandra nie była możliwa zmiana listy rodzin kolumn i przestrzeni kluczy bez ponownego uruchomienia serwera, ale najnowsze wydanie tej bazy wprowadza taką możliwość.

\subsubsection*{Wyszukiwanie rekordów}

Apache Cassandra posiada rozbudowane API pozwalające na filtrowanie i stronicowanie kolumn w ramach pojedynczego rekordu.
Przy wykorzystaniu odpowiedniego mechanizmu partycjonowania, możliwe jest także wyszukiwanie rekordów zakresami po kluczu głównym.

Od wersji 0.7 możliwe jest już tworzenie indeksów drugiego poziomu, co pozwala na wyszukiwanie rekordów po wartości kolumny.
Wcześniej, aby uzyskać taką samą funkcjonalność należało samodzielnie zaimplementować taki indeks poprzez dodanie kolejnej rodziny kolumn, gdzie kluczem byłaby wartość, po której chcemy wyszukać, a nazwami kolumn byłyby identyfikatory wyszukiwanych rekordów.

\subsubsection*{MapReduce}

Cassandra nie posiada własnego, wbudowanego frameworku MapReduce.
Posiada natomiast integrację z systemem Hadoop, dla którego może służyć zarówno jako zbiór danych wejściowych, jak i danych wyjściowych (od wersji 0.7).

\subsection*{Unikalne cechy}

Cassandra łączy w sobie model danych, który jest znacznie bogatszy i łatwiejszy w użyciu niż spotykane w systemach typu klucz-wartość, z technikami znanymi z Amazon Dynamo, które pozwalają na osiągnięcie bardzo wysokiej dostępności systemu.
Dzięki temu, że użytkownikowi jest pozostawiony wybór między konsystencją a dostępnością w obliczu awarii czy podziałów sieci, możliwe jest także stosunkowo proste dopasowanie zachowania systemu do potrzeb aplikacji z niego korzystającej.

Wart uwagi jest fakt, że Apache Cassandra jako jeden z bardzo nielicznych systemów pozwala na taką konfigurację, aby mógł bezproblemowo pracować będąc rozproszonym między wieloma centrami obliczeniowymi, co stawia go w czołówce systemów oferujących najwyższe gwarancje dostępności.

\subsection*{Typowe zastosowania}

Jeszcze zanim ten system został upubliczniony, dowiódł on swojej przydatności w firmie Facebook, gdzie umożliwił wyszukiwanie w miliardach wiadomości które użytkownicy tego serwisu wysyłają do siebie codziennie.
Nie jest zatem przypadkiem, że baza ta została później zaadoptowana przez innych wielkich graczy na rynku aplikacji internetowych: Twitter, Digg i Reddit.
Zainteresowanie jakim Cassandra cieszy się pośród inżynierów największych aplikacji internetowych, świadczy o tym, że system ten należy do grupy najlepiej skalowalnych baz danych dostępnych aktualnie na rynku.

Do tej pory jednak Cassandra nie zdobyła dużej popularności wśród twórców mniejszych aplikacji.
Jest to najprawdopodobniej związane z tym, że tworzenie aplikacji opartych o ten system jest dość trudne i wymaga znacznie większych nakładów pracy niż w przypadku dokumentowych baz danych, czy nawet baz klucz-wartość takich jak Riak, które mogą się pochwalić wbudowanym frameworkiem MapReduce. 
Pomiędzy wersjami 0.6 i 0.7\footnote{W momencie pisania tych słów data wydania wersji 0.7 nie jest jeszcze znana, ale dostępna jest już wersja \emph{Release Candidate} 4.} zostało jednak wprowadzonych wiele zmian (na czele z indeksami drugiego poziomu i możliwością zmiany schematu bez ponownego uruchamiania serwera), które sprawiają, że system ten jest znacznie bardziej przyjazny dla użytkownika niż jeszcze przed rokiem, a zatem jest bardzo prawdopodobne, że nabierze on tym samym większej popularności.

Podobnie jak Riak, Cassandra jest systemem, który powinien dobrze radzić sobie ,,w chmurze''.
Posiada on nawet wbudowane mechanizmy pozwalające na określenie topologii sieci w środowisku Amazon EC2 na potrzeby strategii replikacji.

\subsection*{Przeciwwskazania}

Apache Cassandra jest systemem, w którym odczyty trwają zazwyczaj dłużej niż zapisy, dlatego ten system lepiej się sprawdza w aplikacjach, które wymagają dużej liczby zapisów i modyfikacji.
Znanym ograniczeniem tego systemu jest to, że nie można w nim przechowywać dużych plików, ze względu na to, że protokół Thrift nie udostępnia opcji strumieniowania danych.
Brak transakcji i zegarów wektorowych sprawia także, że problematyczne może być stworzenie w oparciu o ten system aplikacji, w której różni klienci często dokonywaliby zmian tych samych rekordów. 

\subsection*{Dokumentacja i wsparcie}

W internecie jest dostępnych wiele źródeł informacji na temat Apache Cassandra.
Wiele użytecznych informacji znajduje się na wiki projektu, jednak struktura tej strony jest nieprzejrzysta i utrudnia dotarcie do tych danych.

Projekt ten jest aktywnie rozwijany i wyraźnie nabiera co raz większej popularności.

Od dość niedawna możliwe jest też uzyskanie płatnego wsparcia oraz szkoleń dzięki firmie Riptano.

\subsection*{Pomocne odnośniki}

Poniżej zamieszczono kilka odnośników do stron WWW związanych z Apache Cassandra:

\begin{description}
 \item [http://cassandra.apache.org/] - strona domowa projektu
 \item [http://wiki.apache.org/cassandra/] - strona wiki z dokumentacją
 \item [http://www.riptano.com/docs/0.6/index] - alternatywna dokumentacja dla wersji 0.6 oferowana przez firmę Riptano
 \item [http://arin.me/blog/wtf-is-a-supercolumn-cassandra-data-model] - bardzo dobry artykuł opisujący model danych systemu
 \item [http://www.parleys.com/\#st=5\&id=1866] - prezentacja wideo oferująca bardzo dobre wprowadzenie do projektu
\end{description}

\section{HBase}
\label{sec:hbase}

\subsection*{Wstęp}

Apache HBase to system wzorowany na Google BigTable.
Podobnie jak tamten system HBase zależy od dwóch innych systemów: Apache Hadoop i Apache ZooKeeper, pierwszy z których zapewnia replikację (podobnie jak GFS), a drugi przechowuje konfigurację i pozwala na wybór węzła master (tak jak Google Chubby).
HBase powstało początkowo jako rozszerzenie do projektu Hadoop, ale z czasem zostało wydzielone jako osobny projekt, który początkowo przyjął tą samą numerację wersji i harmonogram wydań co projekt-rodzic, ale od najnowszej, jeszcze nie wydanej wersji 0.90, także ta więź zostanie zerwana.

Apache Hadoop, na którym opiera się HBase, to implementacja Google MapReduce w Javie, jednak ponieważ tamten framework zależy od GFS, który nie jest publicznie dostępny, to Hadoop posiada własną implementację rozproszonego systemu plików.

\subsection*{Protokół komunikacji}

Istnieją trzy różne protokoły komunikacji z HBase: Java, Thrift oraz REST (dzięki nakładce o nazwie Stargate).
Najczęściej aktualizowanym i w związku z tym oferującym najwięcej możliwości i najszybciej reagującym na zmiany jest API w Javie, ale pozostałe dwa nie są daleko w tyle.
HBase jest wykorzystywane głównie w połączeniu z aplikacjami pisanymi w języku Java (prawdopodobnie głównie ze względu na ścisłe powiązanie z Apache Hadoop), ale dostępne są też biblioteki dla innych języków programowania.

Podobnie jak większość systemów NoSQL, HBase nie posiada żadnych bardziej zaawansowanych mechanizmów kontroli dostępu.
Jest jednak prawdopodobne, że w przyszłych wersjach taka funkcjonalność zostanie dodana dzięki koprocesorom, czyli funkcjonalności, która ma pojawić się w HBase 0.92 (patrz dalej).
Istnieje już nawet implementacja on nazwie \emph{Secure HBase}, która to umożliwia\footnote{http://hbaseblog.com/2010/10/11/secure-hbase-access-controls/}.

\subsection*{Replikacja}

Podobnie jak Google BigTable, HBase pozostawia replikację rozproszonemu systemowi plików, na którym zapisuje dane.
Teoretycznie, HBase może być wykorzystane z praktycznie dowolnym rozproszonym systemem plików, ale w rzeczywistości system ten jest wykorzystywany tylko w połączeniu z HDFS (\emph{Hadoop Distributed File System}), oraz o wiele rzadziej z KFS (dawniej \emph{Kosmos File System}, teraz \emph{CloudStore}).

\subsubsection*{HDFS}

HDFS jest systemem plików przypominającym swoją architekturą Google File System.
Została ona opisana bliżej w artykule z 2007 roku \cite{hdfs-architecture}, który wprawdzie daje dobry pogląd o tym jak działa ten system, ale jest niestety na tyle stary, że nie jest już autorytatywnym źródłem na ten temat.

Podobnie jak GFS występują w tym systemie dwa typy węzłów: NameNode (jeden w klastrze, odpowiednik węzła Master) oraz DataNode (wszystkie pozostałe, odpowiedniki \emph{chunk-server}).
Tak samo jak GFS, HDFS jest stworzony z myślą przede wszystkim o dużych i bardzo dużych plikach, które podobnie jak w systemie Google są przechowywane podzielone na fragmenty o domyślnej wielkości 64MB, które to fragmenty są także jednostką replikacji.

Największą różnicą między HDFS a GFS jest to, że w systemie Apache awaria węzła NameNode powoduje awarię całego systemu i konieczność jego ponownego uruchomienia.
Jest to o tyle istotne, że węzeł NameNode HDFS pozostaje jedyną ,,piętą Achillesową''\footnote{czyt. \emph{Single Point of Failure}} HBase, które wprawdzie także posiada węzeł Master, ale od niedawna jego awaria już nie powoduje awarii całego systemu.

Inną istotną różnicą między tymi systemami jest to, że ponieważ HDFS powstał na potrzeby narzędzia MapReduce (Hadoop), które z natury nie potrzebuje modyfikować plików po tym jak zostaną one już zapisane, to system ten nie posiada żadnej funkcjonalności przypominającej tą znaną z GFS, która pozwala na dopisywanie ,,rekordów'' na koniec plików przez wielu klientów równocześnie.
Początkowo HDFS był całkowicie pozbawiony możliwości zmieniania zawartości plików, a same pliki były dostępne do odczytu dopiero gdy zostały poprawnie zamknięte\footnote{W związku z tym, HBase jeszcze do niedawna traciło dane w przypadku awarii węzła zapisującego, ponieważ nie dało się ich odczytać z nie zamkniętego poprawnie pliku. W najnowszej wersji (niestabilnej) błąd ten już został naprawiony i da się odzyskiwać dane zapisane w ten sposób.}, później jednak została dodana operacja \verb+append()+, która pozwala na dopisywanie na koniec pliku, ale ponieważ okazało się że nie działa ona całkowicie poprawnie, operacja ta jest dostępna dopiero po ustawieniu odpowiedniej opcji w konfiguracji.

\subsection*{Partycjonowanie}

Apache HBase wykorzystuje ten sam sposób partycjonowania danych co Google BigTable.
Podobnie jak w tamtym systemie, tabele są podzielone na fragmenty o konfigurowalnej wielkości (domyślnie 256MB) nazywane regionami (w BigTable tabletami).
Tak samo jak w BigTable, rzędy są sortowane, ale w odróżnieniu od Apache Cassandra nie jest możliwa zmiana algorytmu sortowania.

Węzły systemu HBase dzielą się na węzeł master oraz serwery regionów (ang. \emph{Region Server}), które są odpowiednikami serwerów tabletów z BigTable.
Każdy serwer regionów (tak jak w Apache Cassandra i BigTable) zapisuje wszystkie zmiany w dzienniku (\verb+HLog+), a następnie odwzorowuje je w pamięci (\verb+MemStore+).
Struktura danych w pamięci jest regularnie zapisywana na dysk do plików (\verb+HFile+), które formatem przypominają pliki SSTable znane z BigTable, a te z kolei co pewien czas są łączone aby uniknąć wyszukiwania w dużej liczbie plików przy odczycie.
Architektura ta jest dokładniej opisana w artykule na blogu Larsa George - jednego z programistów tworzących HBase \cite{george-hbase-storage}, z którego zaczerpnięty został Rysunek \ref{fig:hbase-files}. 

\myfigure{chapters/bazy_kolumnowe/hbase-files.png}{Architektura HBase}{fig:hbase-files}

W starszych wersjach HBase, w przypadku awarii węzła master cały system musiał być uruchomiony ponownie, ale od wersji 0.20 została wprowadzona integracja z odpowiednikiem Google Chubby - Apache ZooKeeper, co pozwoliło na usunięcie tego problemu.

\subsection*{Persystencja}

Za trwałość danych w HBase odpowiada wybrany przez użytkownika system plików.
Jednym z głównych problemów HBase, który został rozwiązany dopiero w nowej, niestabilnej wersji jest wspomniany już problem utraty danych w przypadku awarii serwera regionów, która sprawi, że plik ten nie zostanie poprawnie zamknięty.
Pomijając ten problem, ponieważ każda operacja zmiany wymaga aby dane zostały poprawnie zreplikowane na skonfigurowaną przez użytkownika\footnote{stopień replikacji} liczbę węzłów, to dane zapisane w HBase są zapisywane w sposób trwały w porównaniu z innymi systemami.

\subsection*{Wersjonowanie}

Podobnie jak w Google BigTable, dane są wersjonowane przy pomocy znaczników czasowych (albo innych wartości liczbowych podanych przez użytkownika) i przechowywane są wszystkie wersje wartości, a nie tylko najnowsza jak w Apache Cassandra.

Ponieważ Apache HBase jest systemem CP w rozumieniu Teorii CAP, to przechowywanie wielu wersji każdej wartości ma mniejsze znaczenie niż w systemach typu AP, gdzie konflikty są bardziej prawdopodobne.
Znaczenie to jest tym mniejsze, że HBase posiada możliwość zakładania blokad na rzędy, dzięki czemu możliwe jest zaimplementowanie transakcji w obrębie pojedynczego rekordu (opcjonalnie można też włączyć możliwość blokowania wielu rzędów równocześnie, ale może to się wiązać z negatywnymi skutkami dla wydajności).

\subsection*{Wyszukiwanie}

\subsubsection*{Model Danych}

Model danych HBase jest praktycznie identyczny do modelu danych BigTable\footnote{patrz strona \pageref{google-bigtable-model-danych}.}, oraz w odróżnieniu od Apache Cassandra posługuje się tą samą nomenklaturą nie zmieniając znaczenia poszczególnych terminów.

W HBase jednostką najwyższego poziomu jest \emph{tabela}, która jest posortowanym zbiorem rzędów, z których każdy jest identyfikowany przez klucz, który jest ciągiem bajtów.
Sortowanie odbywa się po kluczu, w porządku leksykalnym, bajtowo.
Każdy rząd może składać się z dowolnej liczby kolumn oraz rodzin kolumn, które z kolei także mogą zawierać kolumny.
Oczywiście tak jak we wszystkich bazach kolumnowych, ,,puste'' wartości nie zajmują miejsca na dysku, a dowolne dwa rzędy mogą się całkowicie różnić zestawem kolumn.

W odróżnieniu od Apache Cassandra nie ma potrzeby deklarowania z góry tabel, mogą one być tworzone dynamicznie, w trakcie działania aplikacji i bez potrzeby restartowania czegokolwiek.

\subsubsection*{Wyszukiwanie rekordów}

Jak przystało na kolumnową bazę, HBase posiada bardzo ograniczone API.
Możliwe jest pobieranie rzędów używając klucza albo zakresu kluczy, a ponadto na poziomie pojedynczego wiersza limitowanie pobranych kolumn (także rodzin kolumn) oraz liczby i zakresu wersji wartości.

Funkcjonalność oferowana przez HBase w zakresie wyszukiwania nie należy do rozbudowanych, stąd też większość ciężaru spoczywa w tym przypadku na użytkowniku, który musi własnoręcznie implementować indeksy umożliwiające znajdowanie rekordów.

\subsection*{Unikalne cechy}

\subsubsection*{Integracja z Apache Hadoop}

Apache Hadoop jest projektem stworzonym z myślą o przetwarzaniu gigantycznych zbiorów danych, a Apache HBase z myślą o przechowywaniu takich zbiorów.
Funkcjonalność oferowana przez ten framework MapReduce stanowi bardzo istotne rozszerzenie dla HBase, ponieważ umożliwia dokonywanie różnego rodzaju obliczeń i transformacji danych, których dane wejściowe i wyjściowe mogą być w niej zapisywane, a co za tym idzie ułatwia tworzenie indeksów, zmaterializowanych widoków, a nawet import i eksport danych.

\subsubsection*{Coprocessor API}

Jedną z ciekawszych funkcjonalności, które będą dostępne w przyszłym wydaniu Apache HBase są koprocesory (ang. \emph{coprocessor}).
Mogą one być porównane do \emph{triggerów} w relacyjnych bazach danych.

Koprocesorem w HBase jest klasa (napisana w Javie), której metody są wywoływane w przestrzeni adresowej serwera w przypadku zajścia różnych zdarzeń.
Do zdarzeń tych należą między innymi zdarzenia związane z tworzeniem i łączeniem plików HFile, oraz zdarzenia, które mają miejsce przy zapisie, odczycie itp. pojedynczych wierszy.
W przyszłości planowane jest także zastosowanie koprocesorów w celu optymalizacji przetwarzania danych na serwerze, w celu zmniejszenia narzutu komunikacyjnego, jaki ma miejsce przy wykorzystaniu Hadoop do wykonywania operacji Map-Reduce.

\subsection*{Typowe zastosowania}

HBase, jako silnie konsystentna baza danych oferuje prostszy model programowania niż na przykład Apache Cassandra, ale z drugiej strony ma także mniejsze możliwości jeżeli chodzi o wyszukiwanie rekordów i filtrowanie kolumn, co sprawia, że nie jest wcale o wiele łatwiejszym systemem w użyciu.
Największą zaletą tej bazy jest bardzo dobra integracja z Apache Hadoop, która czyni ją kuszącą opcją dla aplikacji, które przechowują ogromne ilości danych przetwarzanych na różne sposoby, czyli na przykład wyszukiwarek internetowych.

Do najbardziej znanych aplikacji korzystających z HBase należy StumbleUpon - spersonalizowany silnik rekomendacji różnego rodzaju treści online.
HBase i Hadoop są także wykorzystywane przez wyszukiwarkę internetową firmy Microsoft - Bing\footnote{http://www.theregister.co.uk/2009/05/07/microsoft\_search\_built\_on\_open\_source/}.

\subsection*{Przeciwwskazania}

Apache HBase w swojej obecnej ,,stabilnej'' wersji (0.20.6) nie jest jeszcze bazą wystarczająco stabilną, aby można ją było polecić do zastosowania w środowisku produkcyjnym.
Szczególnie problematyczne jest tu ryzyko utraty danych w przypadku awarii węzła.

Nie wskazane (choć oczywiście możliwe) jest też uruchamianie HBase na platformie Amazon EC2 ze względu na istnienie pojedynczego punktu awarii, a mianowicie węzła NameNode HDFS.

W internecie można znaleźć informacje, że HBase jest zbyt wolne aby nadawać się dla aplikacji on-line\footnote{http://www.metabrew.com/article/anti-rdbms-a-list-of-distributed-key-value-stores/}, jednak przykład StumbleUpon pokazuje, że nie jest to prawdą.

\subsection*{Dokumentacja i wsparcie}

HBase, w odróżnieniu od większości systemów NoSQL nie jest tworzone głównie przez programistów jednej firmy, ale przez osoby należące do wielu różnych instytucji.
Zaletą takiego układu jest to, że łatwiej jest uzyskać darmową pomoc od ludzi, którzy pracują nad tą aplikacją ponieważ ich firmy nie zarabiają na sprzedawaniu tych usług.
Wadą jest jednak to, że brakuje firmy, która oferowałaby wyspecjalizowane usługi, szkolenia i gwarantowane wsparcie dla tego produktu.

Od września 2009 HBase zostało włączone do pakietu produktów opartych o Apache Hadoop oferowanego przez firmę Cloudera, co przynajmniej częściowo mityguje tą wadę.

Dokumentacja projektu, podobnie jak w przypadku Apache Cassandra jest dość chaotyczna i dostępna jedynie w postaci wiki.
W internecie jest dostępnych stosunkowo niewiele wideo-prezentacji opisujących tą bazę, ale za to można znaleźć dość dużo informacji o tym systemie na różnych blogach.

\subsection*{Pomocne odnośniki}

Poniżej zamieszczono kilka odnośników do stron WWW związanych z Apache HBase:

\begin{description}
 \item [http://hbase.apache.org/] - strona domowa projektu
 \item [http://wiki.apache.org/hadoop/Hbase] - strona wiki z dokumentacją
 \item [http://www.parleys.com/parleysserver/indexing/presentation.form?id=1859] - prezentacja wideo oferująca bardzo dobre wprowadzenie do projektu
 \item [http://nosqltapes.com/video/ryan-rawson-on-hbase-at-stumbleupon] - wywiad z jednym z twórców bazy, który opowiada między innymi o zastosowaniach HBase w firmie StumbleUpon
\end{description}
